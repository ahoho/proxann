{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755867091548f53d",
   "metadata": {},
   "source": [
    "# Alt-Test: How to Justify Replacing Humans by LLMs\n",
    "\n",
    "**This notebook has been edited from the original Alt-Test version to apply it to our data -- ProxAnn authors**\n",
    "\n",
    "To run the alt-test, you need two dictionaries: one with human annotations and another with LLM predictions. Then, you should call the `alt_test` function: <br>\n",
    "```python\n",
    "winning_rate, advantage_prob = alt_test(humans_annotations, llm_annotations, scoring_function, epsilon)\n",
    "```\n",
    "\n",
    "The `winning_rate` represents the proportion of humans the LLM \"wins\", and if `winning_rate >= 0.5` the LLM passes the test. <br>\n",
    "The `advantage_prob` estimates the probability that the LLM annotations are as good as or better than a randomly selected human annotator. It should be used to compare LLMs (higher is better). <br>\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`humans_annotations`**:\n",
    "  A dictionary of dictionaries where:\n",
    "  - Outer keys represent annotators (annotator ids).\n",
    "  - Inner dictionaries with keys representing instances (instance ids) and values representing annotations.\n",
    "  **Example:**\n",
    "  ```python\n",
    "  {\n",
    "      'annotator1': {'instance1': 'A', 'instance2': 'B'},\n",
    "      'annotator2': {'instance1': 'A', 'instance2': 'C', 'instance3': 'A'}\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **`llm_annotations`**:\n",
    "  A dictionary where the keys represent instances (instance ids) and the values represent LLM predictions.\n",
    "  **Example:**\n",
    "  ```python\n",
    "  {'instance1': 'A', 'instance2': 'B', 'instance3': 'A'}\n",
    "  ```\n",
    "\n",
    "- **`scoring_function`**:\n",
    "  Specifies how predictions are evaluated. Can be:\n",
    "  - A string: `'accuracy'` or `'neg_rmse'`.\n",
    "  - A custom function: Takes a prediction and a list of annotations as inputs, returning a score.\n",
    "\n",
    "- **`epsilon`**:\n",
    "  A float representing the cost-benefit penalty for the null hypothesis. Suggested values:\n",
    "  - **0.2**: if annotators are experts.\n",
    "  - **0.15**: if annotators are skilled.\n",
    "  - **0.1**: if annotators are crowd-workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:29.616116Z",
     "start_time": "2025-01-18T07:51:29.614151Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from scipy.stats import ttest_1samp, wilcoxon, kendalltau\n",
    "from typing import List, Dict, Any, Callable, Union\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from proxann.llm_annotations.utils import process_responses, collect_fit_rank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503928752382339a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:29.796805Z",
     "start_time": "2025-01-18T07:51:29.785348Z"
    }
   },
   "outputs": [],
   "source": [
    "def open_json(file_path: str) -> Dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def by_procedure(p_values: List[float], q: float) -> List[int]:\n",
    "    p_values = np.array(p_values, dtype=float)\n",
    "    m = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_pvals = p_values[sorted_indices]\n",
    "    # Compute the harmonic sum H_m = 1 + 1/2 + ... + 1/m\n",
    "    H_m = np.sum(1.0 / np.arange(1, m + 1))\n",
    "    # Compute the BY thresholds for each rank i\n",
    "    by_thresholds = (np.arange(1, m + 1) / m) * (q / H_m)\n",
    "    max_i = -1\n",
    "    for i in range(m):\n",
    "        if sorted_pvals[i] <= by_thresholds[i]:\n",
    "            max_i = i\n",
    "    if max_i == -1:\n",
    "        return []\n",
    "    rejected_sorted_indices = sorted_indices[:max_i + 1]\n",
    "    return list(rejected_sorted_indices)\n",
    "\n",
    "\n",
    "def accuracy(pred: Any, annotations: List[Any]) -> float:\n",
    "    return float(np.mean([pred == ann for ann in annotations]))\n",
    "\n",
    "\n",
    "def neg_rmse(pred: Union[int, float], annotations: List[Union[int, float]]) -> float:\n",
    "    return -1 * float(np.sqrt(np.mean([(pred - ann) ** 2 for ann in annotations])))\n",
    "\n",
    "\n",
    "def neg_rmse_topic(pred: list, annotations: List[List[Union[int, float]]]) -> float:\n",
    "    return -1 * float(np.mean([root_mean_squared_error(pred, ann) for ann in annotations]))\n",
    "\n",
    "\n",
    "def tau(pred: list, annotations: List[List[Union[int, float]]]) -> float:\n",
    "    return np.mean([kendalltau(pred, ann)[0] for ann in annotations])\n",
    "\n",
    "\n",
    "def sim(pred: str, annotations: List[str], similarity_func: Callable) -> float:\n",
    "    return float(np.mean([similarity_func(pred, ann) for ann in annotations]))\n",
    "\n",
    "\n",
    "def ttest(indicators, epsilon: float) -> float:\n",
    "    indicators = np.array(indicators, dtype=float)\n",
    "    # avoids warning with standard deviation of zero\n",
    "    if np.all(indicators == 1):\n",
    "        return 1.0\n",
    "    if np.all(indicators == -1):\n",
    "        return 0.0\n",
    "    \n",
    "    return ttest_1samp(indicators, epsilon, alternative='less').pvalue\n",
    "\n",
    "\n",
    "def wilcoxon_test(indicators, epsilon: float) -> float:\n",
    "    # use epsilon\n",
    "    shifted_differences = [d - epsilon for d in indicators]\n",
    "    return wilcoxon(shifted_differences, alternative='less', mode=\"exact\").pvalue\n",
    "\n",
    "\n",
    "def alt_test(\n",
    "    llm_annotations: Dict[Union[int, str], Any],\n",
    "    humans_annotations: Dict[Union[int, str], Dict[Union[int, str], Any]],\n",
    "    scoring_function: Union[str, Callable] = 'accuracy',\n",
    "    epsilon: float = 0.2,\n",
    "    q_fdr: float = 0.05,\n",
    "    min_humans_per_instance: int = 2,\n",
    "    min_instances_per_human: int = 30,\n",
    "    test: str = 'ttest',\n",
    "    verbose: bool = True,\n",
    ") -> float:\n",
    "    # prepare alignment scoring function\n",
    "    if isinstance(scoring_function, str):\n",
    "        if scoring_function == 'accuracy':\n",
    "            scoring_function = accuracy\n",
    "        elif scoring_function == 'neg_rmse':\n",
    "            scoring_function = neg_rmse\n",
    "        elif scoring_function == 'neg_rmse_topic':\n",
    "            scoring_function = neg_rmse_topic\n",
    "        elif scoring_function == 'tau':\n",
    "            scoring_function = lambda pred, human: np.mean([kendalltau(pred, h)[0] for h in human])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown scoring function\")\n",
    "    else:\n",
    "        scoring_function = scoring_function\n",
    "\n",
    "    # prepare sets - i_set has humans as keys, h_set has instances as keys\n",
    "    i_set, h_set = {}, {}\n",
    "    for h, anns in humans_annotations.items():\n",
    "        i_set[h] = list(anns.keys())\n",
    "        for i, ann in anns.items():\n",
    "            if i not in h_set:\n",
    "                h_set[i] = []\n",
    "            h_set[i].append(h)\n",
    "\n",
    "    # remove instances with less than min_humans_per_instance\n",
    "    instances_to_keep = {i for i in h_set if len(h_set[i]) >= min_humans_per_instance and i in llm_annotations}\n",
    "\n",
    "    if len(instances_to_keep) < len(h_set) and verbose:\n",
    "        print(f\"Dropped {len(h_set) - len(instances_to_keep)} instances with less than {min_humans_per_instance} annotators.\")\n",
    "    i_set = {h: [i for i in i_set[h] if i in instances_to_keep] for h in i_set}\n",
    "    h_set = {i: h_set[i] for i in h_set if i in instances_to_keep}\n",
    "\n",
    "    p_values, advantage_probs, humans = [], [], []\n",
    "    for excluded_h in humans_annotations:\n",
    "        llm_indicators = []\n",
    "        excluded_indicators = []\n",
    "        score_differences = []\n",
    "        instances = [i for i in i_set[excluded_h] if i in llm_annotations]\n",
    "        if len(instances) < min_instances_per_human:\n",
    "            if verbose:\n",
    "                print(f\"Skipping annotator {excluded_h} with only {len(instances)} instances < {min_instances_per_human}.\")\n",
    "            continue\n",
    "\n",
    "        avg_rem_ans = 0\n",
    "        for i in instances:\n",
    "            human_ann = humans_annotations[excluded_h][i]\n",
    "            llm_ann = llm_annotations[i]\n",
    "            remaining_anns = [humans_annotations[h][i] for h in h_set[i] if h != excluded_h]\n",
    "            human_score = scoring_function(human_ann, remaining_anns)\n",
    "            llm_score = scoring_function(llm_ann, remaining_anns)\n",
    "            llm_indicators.append(1 if llm_score >= human_score else 0)\n",
    "            excluded_indicators.append(1 if human_score >= llm_score else 0)\n",
    "            score_differences.append(human_score - llm_score)\n",
    "            avg_rem_ans += len(remaining_anns)\n",
    "        avg_rem_ans /= len(instances)\n",
    "        diff_indicators = [exc_ind - llm_ind for exc_ind, llm_ind in zip(excluded_indicators, llm_indicators)]\n",
    "        #print(f\"Mean diff: {np.mean(llm_indicators):0.2f} | Diff n: {len(diff_indicators)} |  Remaining anns n: {avg_rem_ans:0.2f}\")\n",
    "        if test == 'ttest':\n",
    "            global current_data\n",
    "            current_data = diff_indicators\n",
    "            p = ttest(diff_indicators, epsilon)\n",
    "        elif test == 'wilcoxon':\n",
    "            # TODO: change to score_differences?\n",
    "            p = wilcoxon_test(diff_indicators, epsilon)\n",
    "        elif test == 'permutation':\n",
    "            raise NotImplementedError(\"Permutation test not implemented.\")\n",
    "        p_values.append(p)\n",
    "        advantage_probs.append(float(np.mean(llm_indicators)))\n",
    "        humans.append(excluded_h)\n",
    "\n",
    "    rejected_indices = by_procedure(p_values, q_fdr)\n",
    "    advantage_prob = float(np.mean(advantage_probs))\n",
    "    winning_rate = len(rejected_indices) / len(humans)\n",
    "    return winning_rate, advantage_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb4501ecbd58bf",
   "metadata": {},
   "source": [
    "### TM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7bd34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total responses: 121\n",
      "Total responses: 121\n",
      "Removed: 20\n",
      "Total responses: 142\n",
      "Total responses: 142\n",
      "Removed: 25\n"
     ]
    }
   ],
   "source": [
    "data_jsons = [\n",
    "    \"../data/json_out/config_wiki_part1.json\",\n",
    "    \"../data/json_out/config_wiki_part2.json\",\n",
    "    \"../data/json_out/config_bills_part1.json\",\n",
    "    \"../data/json_out/config_bills_part2.json\",\n",
    "]\n",
    "response_csvs = [\n",
    "    \"../data/human_annotations/Cluster+Evaluation+-+Sort+and+Rank+-+Bills_December+14,+2024_13.20.csv\",\n",
    "    \"../data/human_annotations/Cluster+Evaluation+-+Sort+and+Rank_December+12,+2024_05.19.csv\",\n",
    "]\n",
    "start_date = \"2024-12-06 09:00:00\"\n",
    "\n",
    "responses = {}\n",
    "for csv in response_csvs:\n",
    "    for topic_id, topic_responses in process_responses(csv, data_jsons, start_date=start_date, path_save=None, removal_condition=\"loose\").items():\n",
    "        if topic_responses:\n",
    "            responses[topic_id] = topic_responses\n",
    "\n",
    "_, _, _, corr_data = collect_fit_rank_data(responses)\n",
    "corr_data = sorted(corr_data, key=lambda x: x[\"id\"])\n",
    "corr_ids = [x[\"id\"] for x in corr_data]\n",
    "\n",
    "#%% Load the model output data\n",
    "base_path = \"../data/camera_ready_llm_out/mean\"\n",
    "llm_data_patterns = {\n",
    "    \"gpt-4o\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/gpt-4o-2024-08-06/\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/gpt-4o-2024-08-06/\").glob(\"*\")),\n",
    "    },\n",
    "    \"llama-8b\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/Meta-Llama-3.1-8B-Instruct\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/Meta-Llama-3.1-8B-Instruct\").glob(\"*\")),\n",
    "    },\n",
    "    \"llama-70b\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/llama-3.3-70b-instruct-awq/\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/llama-3.3-70b-instruct-awq/\").glob(\"*\")),\n",
    "    },\n",
    "    \"qwen-2.5-72b\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/Qwen2.5-72B-Instruct-AWQ/\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/Qwen2.5-72B-Instruct-AWQ/\").glob(\"*\")),\n",
    "    },\n",
    "    \"qwen-3-8b\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/Qwen3-8B/\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/Qwen3-8B/\").glob(\"*\")),\n",
    "    },\n",
    "    \"qwen-3-32b\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/Qwen3-32B/\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/Qwen3-32B/\").glob(\"*\")),\n",
    "    },\n",
    "    \"qwen-3-30b-moe\": {\n",
    "        \"wiki\": list(Path(base_path, \"wiki/Qwen3-30B-A3B/\").glob(\"*\")),\n",
    "        \"bills\": list(Path(base_path, \"bills/Qwen3-30B-A3B/\").glob(\"*\")),\n",
    "    },\n",
    "}\n",
    "llm_fits, llm_ranks = {}, {}\n",
    "\n",
    "\n",
    "#%% Load the model output data\n",
    "for llm, paths_by_ds in llm_data_patterns.items():\n",
    "    llm_fits[llm] = defaultdict(list)\n",
    "    llm_ranks[llm] = defaultdict(list)\n",
    "\n",
    "    for dataset, paths in paths_by_ds.items():\n",
    "        fits_, ranks_, wins_ = [], [], []\n",
    "        # iterate over all seeds\n",
    "        for seed, path in enumerate(paths):\n",
    "            fits_seed = open_json(f\"{path}/llm_results_q2.json\")\n",
    "            ranks_seed = open_json(f\"{path}/llm_results_q3.json\")\n",
    "\n",
    "            # point is to move from \"seed by topic\"\n",
    "            # [[topic_0_seed_0, topic_1_seed_0, ...], [topic_0_seed_1, topic_1_seed_1, ...]]\n",
    "            # to \"topic by seed\"\n",
    "            # [[topic_0_seed_0, topic_0_seed_1, ...], [topic_1_seed_0, topic_1_seed_1, ...]]\n",
    "            for i, (fit_item, rank_item) in enumerate(zip(fits_seed, ranks_seed)):\n",
    "                assert(fit_item[\"id\"] == rank_item[\"id\"])\n",
    "                if seed == 0:\n",
    "                    fits_.append([fit_item])\n",
    "                    ranks_.append([rank_item])\n",
    "                else:\n",
    "                    fits_[i].append(fit_item)\n",
    "                    ranks_[i].append(rank_item)\n",
    "\n",
    "        # then we can average over all seeds\n",
    "        for fit_item, rank_item in zip(fits_, ranks_):\n",
    "            id = fit_item[0][\"id\"]\n",
    "            llm_fits[llm][dataset].append({\n",
    "                \"id\": id,\n",
    "                \"annotators\": [llm],\n",
    "                \"fit_data\": [np.mean([x[\"fit_data\"][0] for x in fit_item], axis=0).tolist()],\n",
    "            })\n",
    "            llm_ranks[llm][dataset].append({\n",
    "                \"id\": id,\n",
    "                \"annotators\": [llm],\n",
    "                \"rank_data\": [np.mean([x[\"rank_data\"][0] for x in rank_item], axis=0).tolist()],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f116eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotation_data(corr_data, combine_method=None, document_level=True, seed=42):\n",
    "    \"\"\"\n",
    "    Constructs the annotation data for the given correlation data and LLM fits/ranks.\n",
    "    \"\"\"\n",
    "    # TODO: determine if more legitimate to combine over dataset or over model\n",
    "    fit_annotation_data = {\"wiki\": {}, \"bills\": {}}\n",
    "    rank_annotation_data = {\"wiki\": {}, \"bills\": {}}\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for ds in [\"wiki\", \"bills\"]:\n",
    "        for topic_data in corr_data:\n",
    "            topic_id = topic_data[\"id\"]\n",
    "            if ds in topic_id:\n",
    "                n_annotators = topic_data[\"n_annotators\"]\n",
    "                annotator_idxs = rng.permutation(n_annotators)\n",
    "                for id_, i in enumerate(annotator_idxs):\n",
    "                    # combine annotators (this is approved by the paper authors)\n",
    "                    model_id = \"/\".join(topic_id.split(\"/\")[:-1])\n",
    "\n",
    "                    # how to combine multiple annotators \n",
    "                    if combine_method == \"model\":\n",
    "                        annotator_key = f\"{model_id}/ann_{id_}\" # combine annotators per model\n",
    "                    elif combine_method == \"dataset\":\n",
    "                        annotator_key = f\"{ds}/ann_{id_}\" # combine annotators per dataset\n",
    "                    elif combine_method is None:\n",
    "                        annotator_key = f\"{topic_id}/ann_{i}\" # original setup\n",
    "                    \n",
    "                    if annotator_key not in fit_annotation_data[ds]:\n",
    "                        fit_annotation_data[ds][annotator_key] = {}\n",
    "                        rank_annotation_data[ds][annotator_key] = {}\n",
    "                    n_docs = topic_data[\"fit_data\"].shape[1]\n",
    "                    if 1 in topic_data[\"rank_data\"][i] and 0 in topic_data[\"rank_data\"][i]:\n",
    "                        raise ValueError(\"Rank data should not have both 0 and 1.\")\n",
    "                    if document_level:\n",
    "                        for doc in range(n_docs):\n",
    "                            fit_annotation_data[ds][annotator_key][f\"{topic_id}/doc_{doc}\"] = topic_data[\"fit_data\"][i, doc]# >= fit_threshold\n",
    "                            # clip the rank to be between 1 and n_docs\n",
    "                            rank_annotation_data[ds][annotator_key][f\"{topic_id}/doc_{doc}\"] = max(1, topic_data[\"rank_data\"][i, doc])\n",
    "                    else:\n",
    "                        fit_annotation_data[ds][annotator_key][topic_id] = [topic_data[\"fit_data\"][i, doc] for doc in range(n_docs)]\n",
    "                        rank_annotation_data[ds][annotator_key][topic_id] = [max(1, topic_data[\"rank_data\"][i, doc]) for doc in range(n_docs)]\n",
    "\n",
    "    return fit_annotation_data, rank_annotation_data\n",
    "\n",
    "\n",
    "def construct_annotation_data_random(corr_data, topics_to_combine=5, document_level=True, seed=42):\n",
    "    \"\"\"\n",
    "    Constructs the annotation data for the given correlation data and LLM fits/ranks.\n",
    "    \"\"\"\n",
    "    # alternative construction\n",
    "    fit_annotation_data = {\"wiki\": {}, \"bills\": {}}\n",
    "    rank_annotation_data = {\"wiki\": {}, \"bills\": {}}\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # should be around 5, so 35 items per annotator\n",
    "    k = topics_to_combine\n",
    "\n",
    "    for ds in [\"wiki\", \"bills\"]:\n",
    "        corr_data_by_annotator = defaultdict(list)\n",
    "        for topic_data in corr_data:\n",
    "            topic_id = topic_data[\"id\"]\n",
    "            if ds in topic_id:\n",
    "                n_annotators = topic_data[\"n_annotators\"]\n",
    "                for i in range(n_annotators):\n",
    "                    corr_data_by_annotator[topic_id].append({\n",
    "                        \"annotator_id\": i,\n",
    "                        \"fit_data\": topic_data[\"fit_data\"][i],\n",
    "                        \"rank_data\": topic_data[\"rank_data\"][i],\n",
    "                    })\n",
    "\n",
    "        groups = 0\n",
    "        while len(corr_data_by_annotator) > k:\n",
    "            # Sample k topics randomly\n",
    "            topics = list(corr_data_by_annotator.keys())\n",
    "            sampled_topics = rng.choice(topics, size=k, replace=False)\n",
    "\n",
    "            # initialize pseudo-annotator data\n",
    "            annotator_key = f\"{ds}/pseudo_{groups}\"\n",
    "            fit_annotation_data[ds][annotator_key] = {}\n",
    "            rank_annotation_data[ds][annotator_key] = {}\n",
    "            \n",
    "            # Pick random index from each group\n",
    "            for topic_id in sampled_topics:\n",
    "                idx = rng.choice(range(len(corr_data_by_annotator[topic_id])))\n",
    "                topic_data = corr_data_by_annotator[topic_id].pop(idx)\n",
    "                n_docs = len(topic_data[\"fit_data\"])\n",
    "            \n",
    "                if 1 in topic_data[\"rank_data\"] and 0 in topic_data[\"rank_data\"]:\n",
    "                    raise ValueError(\"Rank data should not have both 0 and 1.\")\n",
    "                if document_level:\n",
    "                    for doc in range(n_docs):\n",
    "                        fit_annotation_data[ds][annotator_key][f\"{topic_id}/doc_{doc}\"] = topic_data[\"fit_data\"][doc]\n",
    "                        # clip the rank to be between 1 and n_docs\n",
    "                        rank_annotation_data[ds][annotator_key][f\"{topic_id}/doc_{doc}\"] = max(1, topic_data[\"rank_data\"][doc])\n",
    "                else:\n",
    "                    fit_annotation_data[ds][annotator_key][topic_id] = [topic_data[\"fit_data\"][doc] for doc in range(n_docs)]\n",
    "                    rank_annotation_data[ds][annotator_key][topic_id] = [max(1, topic_data[\"rank_data\"][doc]) for doc in range(n_docs)]\n",
    "                \n",
    "                # remove topic from pool\n",
    "                if len(corr_data_by_annotator[topic_id]) == 0:\n",
    "                    del corr_data_by_annotator[topic_id]\n",
    "\n",
    "            groups += 1\n",
    "    return fit_annotation_data, rank_annotation_data\n",
    "\n",
    "\n",
    "def construct_llm_data(llm_fits, llm_ranks, llm, document_level=True):\n",
    "    \"\"\"\n",
    "    Constructs the LLM data for the given LLM fits and ranks.\n",
    "    \"\"\"\n",
    "    fit_llm_data = {\"wiki\": {}, \"bills\": {}}\n",
    "    rank_llm_data = {\"wiki\": {}, \"bills\": {}}\n",
    "\n",
    "    for ds in [\"wiki\", \"bills\"]:\n",
    "        for topic_fits, topic_ranks in zip(llm_fits[llm][ds], llm_ranks[llm][ds]):\n",
    "            assert topic_fits[\"id\"] == topic_ranks[\"id\"]\n",
    "            topic_id = topic_fits[\"id\"]\n",
    "            if ds in topic_id:\n",
    "                n_docs = len(topic_fits[\"fit_data\"][0])\n",
    "                if document_level:\n",
    "                    for doc in range(n_docs):\n",
    "                        fit_llm_data[ds][f\"{topic_id}/doc_{doc}\"] = topic_fits[\"fit_data\"][0][doc]\n",
    "                        rank_llm_data[ds][f\"{topic_id}/doc_{doc}\"] = topic_ranks[\"rank_data\"][0][doc]\n",
    "                else:\n",
    "                    fit_llm_data[ds][topic_id] = [topic_fits[\"fit_data\"][0][doc] for doc in range(n_docs)]\n",
    "                    rank_llm_data[ds][topic_id] = [topic_ranks[\"rank_data\"][0][doc] for doc in range(n_docs)]\n",
    "    return fit_llm_data, rank_llm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce11de",
   "metadata": {},
   "source": [
    "# Make the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104df348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83951c6e239849f7bf28eaf9d4f2dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "epsilon = 0.1\n",
    "n_models = 3\n",
    "n_topics_per_model = 8\n",
    "n_docs = 7\n",
    "n_resamples = 10\n",
    "topics_to_combine = 8  # for random construction\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for i in tqdm(range(n_resamples), desc=\"Running tests\"):\n",
    "    for doc_level in [True, False]:\n",
    "        if doc_level:\n",
    "            metric = \"neg_rmse\"\n",
    "            n_items = n_docs\n",
    "        else:\n",
    "            metric = \"neg_rmse_topic\"\n",
    "            n_items = 1\n",
    "        \n",
    "        for combine_method in [\"model\", \"dataset\", \"random\"]:\n",
    "            if combine_method == \"model\":\n",
    "                min_instances_per_human = n_topics_per_model * n_items\n",
    "            elif combine_method == \"dataset\":\n",
    "                min_instances_per_human = n_models * n_topics_per_model * n_items\n",
    "            elif combine_method == \"random\":\n",
    "                min_instances_per_human =  topics_to_combine * n_items\n",
    "\n",
    "            for llm in llm_data_patterns:\n",
    "                if combine_method != \"random\":\n",
    "                    fit_annotation_data, rank_annotation_data = construct_annotation_data(\n",
    "                        corr_data, combine_method=combine_method, document_level=doc_level, seed=rng,\n",
    "                    )\n",
    "                else:\n",
    "                    fit_annotation_data, rank_annotation_data = construct_annotation_data_random(\n",
    "                        corr_data, topics_to_combine=topics_to_combine, document_level=doc_level, seed=rng,\n",
    "                    )\n",
    "                fit_llm_data, rank_llm_data = construct_llm_data(\n",
    "                    llm_fits, llm_ranks, llm, document_level=doc_level\n",
    "                )\n",
    "\n",
    "                for ds in [\"wiki\", \"bills\"]:\n",
    "                    for test in [\"ttest\", \"wilcoxon\"]:\n",
    "                        task_data = [\n",
    "                            (\"fit\", fit_llm_data[ds], fit_annotation_data[ds]),\n",
    "                            (\"rank\", rank_llm_data[ds], rank_annotation_data[ds]),\n",
    "                        ]\n",
    "                        for (task_name, task_llm_data, task_annotation_data) in task_data:\n",
    "                            wr, ap = alt_test(\n",
    "                                task_llm_data,\n",
    "                                task_annotation_data,\n",
    "                                metric,\n",
    "                                epsilon=epsilon,\n",
    "                                test=test,\n",
    "                                min_instances_per_human=min_instances_per_human,\n",
    "                                verbose=False,\n",
    "                            )\n",
    "                            results.append({\n",
    "                                \"llm\": llm,\n",
    "                                \"dataset\": ds,\n",
    "                                \"task\": task_name,\n",
    "                                \"combine_method\": combine_method,\n",
    "                                \"doc_level\": doc_level,\n",
    "                                \"test\": test,\n",
    "                                \"iter\": i,\n",
    "                                \"winning_rate\": wr,\n",
    "                                \"advantage_probability\": ap,\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284531ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f0b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_summary_table(results, combine_method=\"dataset\", format_decimals=3, as_decimal=True, show_std=True, multirow=True):\n",
    "    \"\"\"\n",
    "    Create a LaTeX summary table with dataset as outer grouping and both doc/topic levels.\n",
    "    \n",
    "    Parameters:\n",
    "    - results: DataFrame with the analysis results\n",
    "    - combine_method: str, either \"model\" or \"dataset\"\n",
    "    - format_decimals: int, number of decimal places for formatting\n",
    "    - dataset_alignment: str, alignment for dataset column ('c', 'l', 'r')\n",
    "    \n",
    "    Required LaTeX packages: booktabs, multirow\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter results to retain only specific models and rename them\n",
    "    summary = results.copy()\n",
    "    \n",
    "    models_to_retain = {\n",
    "        \"llama-8b\": r\"\\llamaThreeOneEightB{}\",\n",
    "        \"qwen-3-8b\": r\"\\qwenThreeEightB{}\",\n",
    "        \"qwen-3-32b\": r\"\\qwenThreeThirtyTwoB{}\",\n",
    "        \"llama-70b\": r\"\\llamaThreeThreeSeventyB{}\",\n",
    "        \"qwen-2.5-72b\": r\"\\qwenTwoFiveSeventyTwoB{}\",\n",
    "        \"gpt-4o\": r\"\\gptFourO{}\",\n",
    "    }\n",
    "    \n",
    "    summary = summary[\n",
    "        summary[\"llm\"].isin(models_to_retain)\n",
    "        & (summary[\"combine_method\"] == combine_method)\n",
    "    ]\n",
    "    summary[\"llm\"] = summary[\"llm\"].map(models_to_retain)\n",
    "    \n",
    "    # Group and calculate statistics\n",
    "    summary_grouped = []\n",
    "    \n",
    "    for (llm, dataset, doc_level), group in summary.groupby(['llm', 'dataset', 'doc_level']):\n",
    "        row_data = {\n",
    "            'llm': llm,\n",
    "            'dataset': dataset,\n",
    "            'doc_level': doc_level\n",
    "        }\n",
    "        \n",
    "        for task in ['fit', 'rank']:\n",
    "            # Get advantage probability (same for both tests, so we use ttest)\n",
    "            adv_prob = group[group['task'] == task]['advantage_probability']\n",
    "            row_data[f'{task}_adv_mean'] = adv_prob.mean()\n",
    "            row_data[f'{task}_adv_std'] = adv_prob.std()\n",
    "            \n",
    "            # Get winning rates for both tests\n",
    "            ttest_data = group[(group['task'] == task) & (group['test'] == 'ttest')]['winning_rate']\n",
    "            wilcoxon_data = group[(group['task'] == task) & (group['test'] == 'wilcoxon')]['winning_rate']\n",
    "            \n",
    "            row_data[f'{task}_ttest_mean'] = ttest_data.mean() if len(ttest_data) > 0 else 0\n",
    "            row_data[f'{task}_wilcoxon_mean'] = wilcoxon_data.mean() if len(wilcoxon_data) > 0 else 0\n",
    "        \n",
    "        summary_grouped.append(row_data)\n",
    "    \n",
    "    summary_final = pd.DataFrame(summary_grouped)\n",
    "    \n",
    "    # Define model order\n",
    "    model_order = [\"Llama-3.1-8B\", \"Qwen-3-8B\", \"Qwen-3-32B\", \"Llama-3.3-70B\", \"Qwen-2.5-72B\", \"GPT-4o\"]\n",
    "    summary_final['model_order'] = summary_final['llm'].map({m: i for i, m in enumerate(model_order)})\n",
    "    # Define dataset order\n",
    "    dataset_order = [\"wiki\", \"bills\"]\n",
    "    summary_final['dataset_order'] = summary_final['dataset'].map({d: i for i, d in enumerate(dataset_order)})\n",
    "    \n",
    "    # Sort by dataset and model order\n",
    "    summary_final = summary_final.sort_values(['dataset_order', 'model_order'])\n",
    "    \n",
    "    # Create LaTeX table\n",
    "    latex_lines = []\n",
    "    latex_lines.append(rf\"\\begin{{tabular}}{{lllll}}\")\n",
    "    latex_lines.append(r\"\\toprule\")\n",
    "    latex_lines.append(r\"  & \\multicolumn{2}{c}{Document-Level $\\rho$} & \\multicolumn{2}{c}{Topic-Level $\\rho$} \\\\\")\n",
    "    latex_lines.append(r\"\\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\")\n",
    "    latex_lines.append(r\"   & Fit & Rank & Fit & Rank \\\\\")\n",
    "    latex_lines.append(r\"\\midrule\")\n",
    "    \n",
    "    datasets = summary_final['dataset'].unique()\n",
    "    for dataset_idx, dataset in enumerate(datasets):\n",
    "        dataset_data = summary_final[summary_final['dataset'] == dataset]\n",
    "        \n",
    "        # Add dataset header\n",
    "        latex_lines.append(rf\"& \\multicolumn{{4}}{{c}}{{\\texttt{{{dataset}}}}}\\\\\")\n",
    "        latex_lines.append(r\"\\cmidrule(lr){2-5}\")\n",
    "        # Get unique models for this dataset\n",
    "        models_in_dataset = dataset_data['llm'].unique()\n",
    "        n_models = len(models_in_dataset)\n",
    "        \n",
    "\n",
    "        for model_idx, model in enumerate(models_in_dataset):\n",
    "            line_parts = []\n",
    "            \n",
    "            # Model column\n",
    "            line_parts.append(model)\n",
    "            \n",
    "            # Process document-level (True) and topic-level (False) in order\n",
    "            for doc_level in [True, False]:\n",
    "                model_doc_data = dataset_data[(dataset_data['llm'] == model) & (dataset_data['doc_level'] == doc_level)]\n",
    "                \n",
    "                for task in ['fit', 'rank']:\n",
    "                    if len(model_doc_data) > 0:\n",
    "                        row = model_doc_data.iloc[0]\n",
    "                        adv_mean = row[f'{task}_adv_mean']\n",
    "                        adv_std = row[f'{task}_adv_std']\n",
    "                        ttest_mean = row[f'{task}_ttest_mean']\n",
    "                        wilcoxon_mean = row[f'{task}_wilcoxon_mean']\n",
    "                        \n",
    "                        if as_decimal:\n",
    "                            adv_str = f\"${adv_mean:.{format_decimals}f}\"\n",
    "                            if show_std:\n",
    "                                adv_str += rf\"\\ ({adv_std:.{format_decimals}f})\"\n",
    "                        else:\n",
    "                            adv_str = rf\"${adv_mean*100:0.0f}\"\n",
    "                            if show_std:\n",
    "                                adv_str += rf\"\\ ({adv_std*100:0.0f})\"\n",
    "                        markers = \"\"\n",
    "                        if ttest_mean > 0.5 or wilcoxon_mean > 0.5:\n",
    "                            markers = \"^{\"\n",
    "                            if ttest_mean > 0.5:\n",
    "                                markers += \"{*}\"\n",
    "                            if wilcoxon_mean > 0.5:\n",
    "                                markers += r\"\\dagger\"\n",
    "                            markers += \"}\"\n",
    "                        markers += \"$\"\n",
    "                        line_parts.append(f\"{adv_str}{markers}\")\n",
    "                    else:\n",
    "                        # Handle missing data\n",
    "                        line_parts.append(\"--\")\n",
    "            \n",
    "            line = \" & \".join(line_parts) + r\" \\\\\"\n",
    "            latex_lines.append(line)\n",
    "        \n",
    "        if dataset_idx < len(datasets) - 1:\n",
    "            latex_lines.append(r\"\\midrule\")\n",
    "    \n",
    "    latex_lines.append(r\"\\bottomrule\")\n",
    "    latex_lines.append(r\"\\end{tabular}\")\n",
    "    \n",
    "    return \"\\n\".join(latex_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79800ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "  & \\multicolumn{2}{c}{Document-Level $\\rho$} & \\multicolumn{2}{c}{Topic-Level $\\rho$} \\\\\n",
      "\\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n",
      "   & Fit & Rank & Fit & Rank \\\\\n",
      "\\midrule\n",
      "& \\multicolumn{4}{c}{\\texttt{wiki}}\\\\\n",
      "\\cmidrule(lr){2-5}\n",
      "\\gptFourO{} & $0.56^{{*}\\dagger}$ & $0.68^{{*}\\dagger}$ & $0.66^{\\dagger}$ & $0.55^{\\dagger}$ \\\\\n",
      "\\llamaThreeOneEightB{} & $0.22$ & $0.36$ & $0.05$ & $0.11$ \\\\\n",
      "\\llamaThreeThreeSeventyB{} & $0.57^{{*}\\dagger}$ & $0.67^{{*}\\dagger}$ & $0.58^{\\dagger}$ & $0.50^{\\dagger}$ \\\\\n",
      "\\qwenThreeEightB{} & $0.56^{{*}\\dagger}$ & $0.58^{\\dagger}$ & $0.46$ & $0.39$ \\\\\n",
      "\\qwenThreeThirtyTwoB{} & $0.55^{{*}\\dagger}$ & $0.63^{\\dagger}$ & $0.47$ & $0.42$ \\\\\n",
      "\\qwenTwoFiveSeventyTwoB{} & $0.52^{\\dagger}$ & $0.68^{{*}\\dagger}$ & $0.66^{\\dagger}$ & $0.46$ \\\\\n",
      "\\midrule\n",
      "& \\multicolumn{4}{c}{\\texttt{bills}}\\\\\n",
      "\\cmidrule(lr){2-5}\n",
      "\\gptFourO{} & $0.65^{{*}\\dagger}$ & $0.71^{{*}\\dagger}$ & $0.77^{{*}\\dagger}$ & $0.75^{{*}\\dagger}$ \\\\\n",
      "\\llamaThreeOneEightB{} & $0.30$ & $0.53^{\\dagger}$ & $0.14$ & $0.44$ \\\\\n",
      "\\llamaThreeThreeSeventyB{} & $0.66^{{*}\\dagger}$ & $0.67^{{*}\\dagger}$ & $0.70^{{*}\\dagger}$ & $0.60^{\\dagger}$ \\\\\n",
      "\\qwenThreeEightB{} & $0.66^{{*}\\dagger}$ & $0.57^{\\dagger}$ & $0.80^{{*}\\dagger}$ & $0.43$ \\\\\n",
      "\\qwenThreeThirtyTwoB{} & $0.67^{{*}\\dagger}$ & $0.68^{{*}\\dagger}$ & $0.74^{{*}\\dagger}$ & $0.70^{{*}\\dagger}$ \\\\\n",
      "\\qwenTwoFiveSeventyTwoB{} & $0.61^{{*}\\dagger}$ & $0.71^{{*}\\dagger}$ & $0.78^{{*}\\dagger}$ & $0.65^{\\dagger}$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# pseudo-annotators by dataset\n",
    "print(create_latex_summary_table(\n",
    "    results,\n",
    "    combine_method=\"dataset\",\n",
    "    format_decimals=2,\n",
    "    as_decimal=True,\n",
    "    show_std=False,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1599320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "  & \\multicolumn{2}{c}{Document-Level $\\rho$} & \\multicolumn{2}{c}{Topic-Level $\\rho$} \\\\\n",
      "\\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n",
      "   & Fit & Rank & Fit & Rank \\\\\n",
      "\\midrule\n",
      "& \\multicolumn{4}{c}{\\texttt{wiki}}\\\\\n",
      "\\cmidrule(lr){2-5}\n",
      "\\gptFourO{} & $0.58^{\\dagger}$ & $0.68^{\\dagger}$ & $0.67$ & $0.55$ \\\\\n",
      "\\llamaThreeOneEightB{} & $0.22$ & $0.37$ & $0.04$ & $0.12$ \\\\\n",
      "\\llamaThreeThreeSeventyB{} & $0.58^{\\dagger}$ & $0.67^{\\dagger}$ & $0.58$ & $0.48$ \\\\\n",
      "\\qwenThreeEightB{} & $0.58^{\\dagger}$ & $0.58$ & $0.46$ & $0.38$ \\\\\n",
      "\\qwenThreeThirtyTwoB{} & $0.57^{\\dagger}$ & $0.63$ & $0.51$ & $0.42$ \\\\\n",
      "\\qwenTwoFiveSeventyTwoB{} & $0.53^{\\dagger}$ & $0.68^{\\dagger}$ & $0.67$ & $0.46$ \\\\\n",
      "\\midrule\n",
      "& \\multicolumn{4}{c}{\\texttt{bills}}\\\\\n",
      "\\cmidrule(lr){2-5}\n",
      "\\gptFourO{} & $0.65^{{*}\\dagger}$ & $0.71^{{*}\\dagger}$ & $0.77$ & $0.76$ \\\\\n",
      "\\llamaThreeOneEightB{} & $0.30$ & $0.53^{\\dagger}$ & $0.14$ & $0.44$ \\\\\n",
      "\\llamaThreeThreeSeventyB{} & $0.66^{{*}\\dagger}$ & $0.67^{\\dagger}$ & $0.69$ & $0.60$ \\\\\n",
      "\\qwenThreeEightB{} & $0.66^{{*}\\dagger}$ & $0.57^{\\dagger}$ & $0.80^{{*}}$ & $0.43$ \\\\\n",
      "\\qwenThreeThirtyTwoB{} & $0.67^{{*}\\dagger}$ & $0.68^{\\dagger}$ & $0.75$ & $0.71$ \\\\\n",
      "\\qwenTwoFiveSeventyTwoB{} & $0.61^{{*}\\dagger}$ & $0.70^{\\dagger}$ & $0.79$ & $0.65$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# pseudo-annotators by model\n",
    "print(create_latex_summary_table(\n",
    "    results,\n",
    "    combine_method=\"model\",\n",
    "    format_decimals=2,\n",
    "    as_decimal=True,\n",
    "    show_std=False,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proxann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
