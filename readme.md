# THETA-EVALUATION

## Get Data for Pilot

Configure the `config/config_pilot.conf` file and execute one of the following scripts:

- `bash_scripts/jsonfy_pilot_wmd.sh` (to make alignment based on WMD, only works when given 2 models)
- `bash_scripts/jsonfy_pilot.sh` (based on Jensen-Shannon and the Hungarian algorithm, works for N models)

These scripts generate the following output, which is also saved at `data/json_out/config_pilot.json`:

```json
 {  
    "<model_path>": {
        "<topic_id>": {
            "topic_words": ["word1", "word2", "word3", ...], 
            "exemplar_docs": ["doc1", "doc2", "doc3", ...], 
            "eval_docs": [
                {
                    "doc_id": 1, 
                    "text": "Document text goes here.", 
                    "prob": 0.9, 
                    "assigned_to_k": 1
                },
                {
                    "doc_id": 2, 
                    "text": "Document text goes here.", 
                    "prob": 0.8, 
                    "assigned_to_k": 1
                },
                ...
            ],
            "distractor_doc": {
                "doc_id": 100,
                "text": "Document text goes here"
            }
        },
        ...
    }
}
```

## Modules

### Embedder

Calculates embeddings using `SentenceTransformers`, with or without aggregation, and saves them as strings in a new dataframe column.

### TopicJsonFormatter

Generates JSON output for topic models, including:

- **Representative Documents** (methods: `'thetas'`, `'thetas_sample'`, `'thetas_thr'`, `'sall'`, `'spart'`, `'s3'`)
- **Top Words for Each Topic**
- **Evaluation Documents and Their Probabilities**

Example structure:

```json
{
    "<topic_id>": {
        "topic_words": ["word1", "word2", "word3", ...], 
        "exemplar_docs": ["doc1", "doc2", "doc3", ...], 
        "eval_docs": [
            {
                "doc_id": 1, 
                "text": "Document text goes here.", 
                "prob": 0.9, 
                "assigned_to_k": 1
            },
            {
                "doc_id": 2, 
                "text": "Document text goes here.", 
                "prob": 0.8, 
                "assigned_to_k": 1
            },
            ...
        ],
        "distractor_doc": {
            "doc_id": 100,
            "text": "Document text goes here"
        }
    },
    ...
}
```

### DocSelector

Implements methods to select representative documents for each topic:

- **thetas**: Selects the ntop-documents with the highest thetas (doc-topic distrib).
- **thetas_sample**: The ntop-documents are selected based on probabilistic sampling. The thetas matrix is normalized such that the columns sum to 1. For each topic, documents are sampled according to their probabilities in the normalized thetas matrix.
- **thetas_thr**: The ntop-documents are selected based on a threshold. The thetas matrix is filtered such that only values within the specified threshold range are kept.
- **sall**: Top docs are selected based on the largest Bhattacharya coefficient between their normalized BoW and the betas.
- **spart**: Top docs are chosen by identifying those with the largest Bhattacharya coefficient between the BoW of the document, specific to the words generated for the topic, and the topic's betas.
- **s3**: For each topic, top docs are chosen by keeping those with the largest sum of the weights that such a topic assigns to each word in the document.
- **elbow**: Selects top docs by identifying the elbow point for each topic. Documents are chosen based on the probabilities in the thetas matrix after filtering out values below the elbow point.

### TopicSelector

Implements methods to find optimal pairings between topics from different models.

### TMTrainer

Provides different topic modeling wrappers: ``LDA-Mallet``, ``LDA-Tomotopy``, and ``BERTopic``.

> **IMPORTANT TO RUN LDA-Mallet**
>
> Download the [latest release of Mallet](https://github.com/mimno/Mallet/releases) and place it in the `src/train` directory. This can be done using the script `bash_scripts/wget_mallet.sh`.

## Running the Modules

All modules can be run via the `main.py` file.

**Example usage:**

```bash
python main.py generate_embeddings --source_file=<path> --output_file=<path> --batch_size=128
python main.py train_tm --corpus_file=<path> --model_path=<path> --trainer_type=MalletLda --num_topics=50
python main.py get_top_docs --method=thetas --thetas_path=<path> --ntop=5
python main.py jsonfy --method=thetas --thetas_path=<path> --ntop=5
```

The ``bash_scripts`` folder contains one bash script per functionality on how to run them.

## Notebooks

- **`bertopic_output.ipynb`**: Displays the topics generated by different BERTopic models.
- **`lda_mallet_vs_tomotopy.ipynb`**: Compares the outputs of a 50-topic LDA model trained on Wikipedia using Mallet and Tomotopy.
- **`lda_mallet_vs_tomotopy.ipynb`**: Calculates the top-5 documents using the thetas, thetas sampling, and thetas thresholding criteria.
- **`elbow_method.ipynb.ipynb`**: Computes and plots document-topic distributions, and identifies elbow points to determine significant documents for each topic.
- **`check_matching.ipynb`**: Shows keys of matched topics among different topic models.